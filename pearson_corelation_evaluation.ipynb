{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89714791",
   "metadata": {},
   "source": [
    "Within this notebook we evaluate the performance of GUM-Net on the FVC2004 dataset and on our custom Kaggle dataset.\n",
    "As alignment measure we use the Pearson Correlation Coefficient (PCC) $$\n",
    "\\rho_{X,Y} = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\,\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}$$ between a template X (Sa) and its (warped) impression Y (Sb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b5147",
   "metadata": {},
   "source": [
    "## Dataset paths and Dataloaders\n",
    "We need a single path to a database for the FVC2004 dataset and both a path to a master template and a path to its impressions for our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a817435",
   "metadata": {},
   "outputs": [],
   "source": [
    "fvc_path = \"data/FVC/FVC2004/Dbs/DB1_B\"\n",
    "custom_impressions_path = \"data/Kaggle/data/5x5000/Finger_1\"\n",
    "custom_template_path = \"data/Kaggle/data/5x5000/Master_Templates/1.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "458868c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.DB1_data import get_set_eval_dataloader # Switch to DB2_data/DB3_data is possible\n",
    "from datasets.kaggle_data import get_eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75075aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 samples (no split).\n",
      "Loaded 5000 samples (no split).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "FVC_loader = get_set_eval_dataloader(\n",
    "    data_root=fvc_path,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    num_images=10*8, # Be aware: Set A databases contain 100*8 images, set B databases only 10*8\n",
    ")\n",
    "Kaggle_loader = get_eval_dataloader(\n",
    "    data_root=custom_impressions_path,\n",
    "    batch_size=100,\n",
    "    num_workers=0,\n",
    ")\n",
    "template_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Pad(padding=(-7, -70, -8, -70), fill=255),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.RandomInvert(p=1.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "template_image = Image.open(custom_template_path).convert(\"L\")\n",
    "template_tensor: torch.Tensor = template_transform(template_image) # type: ignore\n",
    "template = template_tensor.unsqueeze(0).repeat(100, 1, 1, 1)[:100] # Set this to the same batch size as the Kaggle dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b043517",
   "metadata": {},
   "source": [
    "## Variables and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efcbe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.losses.pearson_correlation_loss import PearsonCorrelationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8904e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_correlation = []\n",
    "cross_correlation_warped = []\n",
    "loss = PearsonCorrelationLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def plot_side_by_side(image_left, image_middle, image_right, title_left=\"Template\", title_middle=\"Impression\", title_right=\"Warped\"):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(8, 4))\n",
    "    axes[0].imshow(image_left, cmap=\"gray\")\n",
    "    axes[0].set_title(title_left)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(image_middle, cmap=\"gray\")\n",
    "    axes[1].set_title(title_middle)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(image_right, cmap=\"gray\")\n",
    "    axes[2].set_title(title_right)\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def per_pair_cross_correlation(Sa, Sb_CC, warped_Sb):\n",
    "    before = loss(Sa, Sb_CC)\n",
    "    after = loss(Sa, warped_Sb)\n",
    "    cross_correlation.append(before)\n",
    "    cross_correlation_warped.append(after)\n",
    "    print(f\"Batch {idx}: Before CC={before:.4f}, After CC={after:.4f}\")\n",
    "\n",
    "\n",
    "def tensor_to_uint8(t):\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.detach()\n",
    "\n",
    "    if t.dim() == 4:\n",
    "        batch = t\n",
    "    elif t.dim() == 3:\n",
    "        batch = t.unsqueeze(0)\n",
    "    elif t.dim() == 2:\n",
    "        batch = t.unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported tensor shape: {tuple(t.shape)}\")\n",
    "\n",
    "    # Undo normalization (mean=0.5, std=0.5)\n",
    "    batch = batch * 0.5 + 0.5\n",
    "    batch = torch.clamp(batch, 0, 1)\n",
    "    imgs = (batch.squeeze(1).cpu().numpy() * 255).astype(np.uint8)\n",
    "    return imgs\n",
    "\n",
    "def align_with_orb(template_tensor, impression_tensor):\n",
    "    templates = tensor_to_uint8(template_tensor)\n",
    "    impressions = tensor_to_uint8(impression_tensor)\n",
    "\n",
    "    if templates.ndim == 2:\n",
    "        templates = templates[None, ...]\n",
    "    if impressions.ndim == 2:\n",
    "        impressions = impressions[None, ...]\n",
    "\n",
    "    if templates.shape[0] != impressions.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Batch size mismatch: templates={templates.shape[0]} impressions={impressions.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    aligned_list = []\n",
    "    for i in range(templates.shape[0]):\n",
    "        template = templates[i]\n",
    "        impression = impressions[i]\n",
    "\n",
    "        orb = cv2.ORB_create(2000)\n",
    "\n",
    "        kp1, des1 = orb.detectAndCompute(template, None)\n",
    "        kp2, des2 = orb.detectAndCompute(impression, None)\n",
    "\n",
    "        aligned = impression\n",
    "        if des1 is not None and des2 is not None:\n",
    "            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "            matches = bf.match(des1, des2)\n",
    "\n",
    "            if len(matches) >= 10:\n",
    "                matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "                src_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2) # type: ignore\n",
    "                dst_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2) # type: ignore\n",
    "\n",
    "                # Affine transform (better for fingerprints than homography)\n",
    "                M, mask = cv2.estimateAffinePartial2D(src_pts, dst_pts, method=cv2.RANSAC)\n",
    "\n",
    "                if M is not None:\n",
    "                    aligned = cv2.warpAffine(impression, M, (template.shape[1], template.shape[0]))\n",
    "\n",
    "        aligned_list.append(aligned)\n",
    "\n",
    "    aligned_np = np.stack(aligned_list, axis=0)\n",
    "    aligned_tensor = torch.from_numpy(aligned_np).float() / 255.0\n",
    "    aligned_tensor = (aligned_tensor - 0.5) / 0.5\n",
    "    aligned_tensor = aligned_tensor.unsqueeze(1).to(template_tensor.device)\n",
    "    return aligned_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73632bf",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028073be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.gumnet import GumNet\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0155448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gumnet(device: Union[str, torch.device] = \"cpu\"):\n",
    "    ckpt_path = \"model/gumnet_2d_best_noise_level_0_8x8_200.pth.zip\" # Use the checkpoints provided in the repository\n",
    "    model = GumNet(grid_size=8)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca914383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GumNet(\n",
       "  (feature_extractor): GumNetFeatureExtraction(\n",
       "    (shared_conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1_sa): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn1_sb): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool1): DCTSpectralPooling(\n",
       "      (dct_h): LinearDCT(in_features=190, out_features=190, bias=False)\n",
       "      (dct_w): LinearDCT(in_features=190, out_features=190, bias=False)\n",
       "      (idct_h): LinearDCT(in_features=100, out_features=100, bias=False)\n",
       "      (idct_w): LinearDCT(in_features=100, out_features=100, bias=False)\n",
       "    )\n",
       "    (shared_conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2_sa): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2_sb): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool2): DCTSpectralPooling(\n",
       "      (dct_h): LinearDCT(in_features=98, out_features=98, bias=False)\n",
       "      (dct_w): LinearDCT(in_features=98, out_features=98, bias=False)\n",
       "      (idct_h): LinearDCT(in_features=50, out_features=50, bias=False)\n",
       "      (idct_w): LinearDCT(in_features=50, out_features=50, bias=False)\n",
       "    )\n",
       "    (shared_conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn3_sa): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3_sb): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool3): DCTSpectralPooling(\n",
       "      (dct_h): LinearDCT(in_features=48, out_features=48, bias=False)\n",
       "      (dct_w): LinearDCT(in_features=48, out_features=48, bias=False)\n",
       "      (idct_h): LinearDCT(in_features=25, out_features=25, bias=False)\n",
       "      (idct_w): LinearDCT(in_features=25, out_features=25, bias=False)\n",
       "    )\n",
       "    (shared_conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn4_sa): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4_sb): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool4): DCTSpectralPooling(\n",
       "      (dct_h): LinearDCT(in_features=23, out_features=23, bias=False)\n",
       "      (dct_w): LinearDCT(in_features=23, out_features=23, bias=False)\n",
       "      (idct_h): LinearDCT(in_features=16, out_features=16, bias=False)\n",
       "      (idct_w): LinearDCT(in_features=16, out_features=16, bias=False)\n",
       "    )\n",
       "    (shared_conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn5_sa): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5_sb): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (l2_norm): FeatureL2Norm()\n",
       "  )\n",
       "  (siamese_matcher): GumNetSiameseMatching(\n",
       "    (correlation_layer): FeatureCorrelation2D()\n",
       "    (regression_block_ab): Sequential(\n",
       "      (0): Conv2d(196, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (13): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU(inplace=True)\n",
       "      (15): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (16): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (regression_block_ba): Sequential(\n",
       "      (0): Conv2d(196, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (13): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU(inplace=True)\n",
       "      (15): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (16): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (l2_norm): FeatureL2Norm()\n",
       "  )\n",
       "  (spatial_aligner): GumNetNonLinearAlignment(\n",
       "    (fc1): Linear(in_features=8192, out_features=2000, bias=True)\n",
       "    (fc2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (fc_out): Linear(in_features=2000, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = init_gumnet(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0edb6d",
   "metadata": {},
   "source": [
    "## Running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Before CC=0.6553, After CC=0.7212\n",
      "Batch 1: Before CC=0.7477, After CC=0.8469\n",
      "Batch 2: Before CC=0.6341, After CC=0.7251\n",
      "Batch 3: Before CC=0.6399, After CC=0.7346\n",
      "Batch 4: Before CC=0.7359, After CC=0.8202\n",
      "Batch 5: Before CC=0.7208, After CC=0.7861\n",
      "Batch 6: Before CC=0.7931, After CC=0.8620\n",
      "Batch 7: Before CC=0.6491, After CC=0.7154\n",
      "Batch 8: Before CC=0.6977, After CC=0.7619\n",
      "Batch 9: Before CC=0.6850, After CC=0.7827\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(FVC_loader):\n",
    "    #Sa = template.to(device) # Uncomment and switch the dataloder to use the Kaggle template\n",
    "    Sa = batch[\"Sa\"].to(device)\n",
    "    Sb = batch[\"Sb\"].to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        warped_Sb, control_points = model(Sa, Sb)\n",
    "    \n",
    "    #warped_Sb = align_with_orb(Sa, warped_Sb) # Uncomment to apply ORB-based alignment\n",
    "    \n",
    "    per_pair_cross_correlation(Sa, Sb, warped_Sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d73d02",
   "metadata": {},
   "source": [
    "## Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9cb7e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-Correlation Before Warping: 0.6959\n",
      "Average Cross-Correlation After Warping: 0.7756\n",
      "Percentage Improvement: 11.46%\n"
     ]
    }
   ],
   "source": [
    "def print_CC():\n",
    "    total_before = sum(cross_correlation) / len(cross_correlation)\n",
    "    total_after = sum(cross_correlation_warped) / len(cross_correlation_warped)\n",
    "    percentage_improvement = ((total_after - total_before) / total_before) * 100\n",
    "    print(f\"Average Cross-Correlation Before Warping: {total_before:.4f}\")\n",
    "    print(f\"Average Cross-Correlation After Warping: {total_after:.4f}\")\n",
    "    print(f\"Percentage Improvement: {percentage_improvement:.2f}%\")\n",
    "\n",
    "print_CC()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
