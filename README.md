# 2D Gum-Net

### Unsupervised Dense Deformation Grid Prediction for Elastic Fingerprint Alignment

<p align=\"center\">
  <a href=\"https://cvpr.thecvf.com/\"><img src=\"https://img.shields.io/badge/CVPR-2026-blue?style=for-the-badge\" alt=\"CVPR 2026\"></a>
  <a href=\"#\"><img src=\"https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white\" alt=\"PyTorch\"></a>
  <a href=\"#license\"><img src=\"https://img.shields.io/badge/License-MIT-green?style=for-the-badge\" alt=\"License\"></a>
  <a href=\"#\"><img src=\"https://img.shields.io/badge/Python-3.9+-3776ab?style=for-the-badge&logo=python&logoColor=white\" alt=\"Python\"></a>
</p>

<p align=\"center\">
  <a href=\"#-overview\">Overview</a> â€¢
  <a href=\"#-key-contributions\">Contributions</a> â€¢
  <a href=\"#-architecture\">Architecture</a> â€¢
  <a href=\"#-results\">Results</a> â€¢
  <a href=\"#-quick-start\">Quick Start</a> â€¢
  <a href=\"#-citation\">Citation</a>
</p>

<br>

**TL;DR:** We introduce an unsupervised deep learning framework that predicts dense deformation fields for elastic fingerprint alignmentâ€”eliminating the need for ground-truth labels while significantly reducing False Rejection Rates.

<br>

<img src=\"assets/gumnet_pipeline.png\" alt=\"2D Gum-Net Pipeline\" width=\"95%\"/>

<sub><b>Figure 1.</b> The 2D Gum-Net pipeline: Given a template S<sub>a</sub> and distorted impression S<sub>b</sub>, our framework extracts features via DCT spectral pooling, computes bidirectional correlation maps, and predicts a dense deformation field for pixel-wise alignmentâ€”all without supervision.</sub>

</div>

<br>

---

## ğŸ¯ Overview

> *\"The fundamental challenge in fingerprint recognition isn't matchingâ€”it's alignment.\"*

Elastic skin deformation during capture introduces **non-linear distortions** that cause genuine fingerprint pairs to appear different. Traditional approaches either:
- Require expensive ground-truth deformation labels (impractical to obtain)
- Rely on minutiae extraction (fails under severe distortion)
- Use rigid transformations (insufficient for elastic deformation)

**2D Gum-Net** solves this by learning to predict dense deformation fields in a completely **unsupervised** manner, adapting techniques from structural biology (cryo-electron tomography) to the biometrics domain.

<br>

## âœ¨ Key Contributions

<table>
<tr>
<td width=\"50%\" valign=\"top\">

### 1ï¸âƒ£ Cross-Domain Adaptation
First successful adaptation of 3D subtomogram alignment networks to 2D fingerprint biometrics, demonstrating that unsupervised structural alignment principles generalize across vastly different domains.

### 2ï¸âƒ£ Unsupervised DDF Prediction
Novel Dense Spatial Transformer (DST) module enables end-to-end learning of pixel-wise deformation fields without any ground-truth supervisionâ€”solving the label acquisition bottleneck.

</td>
<td width=\"50%\" valign=\"top\">

### 3ï¸âƒ£ DCT Spectral Pooling
Frequency-domain pooling layers that preserve ridge and valley microstructure significantly better than conventional spatial pooling, critical for maintaining discriminative fingerprint features.

### 4ï¸âƒ£ Robust Pre-Match Rectification  
Effective mitigation of elastic torque and non-linear stretching, enabling substantial reduction in False Rejection Rate (FRR) even under severe distortion conditions.

</td>
</tr>
</table>

<br>

---

## ğŸ—ï¸ Architecture

### Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           2D Gum-Net Framework                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ Template â”‚â”€â”€â”€â”€â”€â–¶â”‚                  â”‚      â”‚                       â”‚    â”‚
â”‚   â”‚   Sâ‚     â”‚      â”‚     Feature      â”‚â”€â”€â”€â”€â”€â–¶â”‚   Siamese Matching    â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    Extraction    â”‚      â”‚                       â”‚    â”‚
â”‚                     â”‚   + DCT Pooling  â”‚      â”‚  â€¢ Correlation Maps   â”‚    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚                  â”‚â”€â”€â”€â”€â”€â–¶â”‚  â€¢ Câ‚áµ¦, Cáµ¦â‚          â”‚    â”‚
â”‚   â”‚Impressionâ”‚â”€â”€â”€â”€â”€â–¶â”‚                  â”‚      â”‚                       â”‚    â”‚
â”‚   â”‚   Sáµ¦     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚                â”‚
â”‚                                                           â–¼                â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  Dense Spatial        â”‚     â”‚
â”‚   â”‚ Aligned  â”‚â—€â”€â”€â”€â”€â”€â”‚  Deformation     â”‚â—€â”€â”€â”€â”€â”‚  Transformer (DST)    â”‚     â”‚
â”‚   â”‚   Sáµ¦'    â”‚      â”‚  Field Î¦        â”‚     â”‚                       â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â€¢ Control Points     â”‚     â”‚
â”‚                                              â”‚  â€¢ Bicubic Interp.    â”‚     â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Module Details

| Module | Components | Output |
|:-------|:-----------|:-------|
| **Feature Extraction** | CNN backbone + DCT spectral pooling layers | Feature maps v<sub>a</sub>, v<sub>b</sub> |
| **Siamese Matching** | Symmetric 2D correlation layer | Bidirectional correlation maps C<sub>ab</sub>, C<sub>ba</sub> |
| **Non-linear Alignment** | Control point regression + Bicubic interpolation | Dense deformation field Î¦ |
| **Spatial Transformer** | Differentiable grid sampling | Aligned impression S<sub>b</sub>' |

<br>

### ğŸ”¬ DCT Spectral Pooling: Why It Matters

<div align=\"center\">
<img src=\"assets/dct_viz.png\" alt=\"DCT Spectral Pooling Comparison\" width=\"90%\"/>

<sub><b>Figure 2.</b> Comparison of pooling methods across subsampling factors (1:1 â†’ 16:16). DCT spectral pooling preserves ridge structure and fine details significantly better than max or average pooling at aggressive downsampling ratesâ€”critical for maintaining discriminative fingerprint features.</sub>
</div>

<br>

**The Problem:** Standard pooling operations (max, average) destroy high-frequency information essential for fingerprint matching.

**Our Solution:** DCT spectral pooling operates in the frequency domain, selectively retaining the most informative coefficients:

```python
# Conceptual DCT Spectral Pooling
def dct_spectral_pool(x, output_size):
    # Transform to frequency domain
    X_dct = dct_2d(x)
    
    # Crop to retain low-frequency components
    X_cropped = X_dct[:, :, :output_size[0], :output_size[1]]
    
    # Transform back to spatial domain
    return idct_2d(X_cropped)
```

<br>

### Training Objective

The network optimizes an unsupervised objective combining structural alignment and spatial smoothness:

$$\mathcal{L}_{	ext{total}} = \mathcal{L}_{	ext{dice}} + \lambda \cdot \mathcal{L}_{	ext{smooth}}$$

| Loss Component | Description | Purpose |
|:---------------|:------------|:--------|
| $\mathcal{L}_{	ext{dice}}$ | Soft Dice coefficient between aligned S<sub>b</sub>' and template S<sub>a</sub> | Maximize structural overlap |
| $\mathcal{L}_{	ext{smooth}}$ | Spatial gradient penalty on deformation field Î¦ | Enforce physically plausible deformations |

<br>

---

## ğŸ“Š Results

### Alignment Performance

Our method demonstrates robust alignment across diverse conditions:

<table>
<tr>
<td align=\"center\"><b>âœ“ High Distortion Tolerance</b><br><sub>Maintains accuracy under severe elastic deformation</sub></td>
<td align=\"center\"><b>âœ“ Noise Robustness</b><br><sub>Consistent across 5 SNR levels</sub></td>
<td align=\"center\"><b>âœ“ Universal Generalization</b><br><sub>Effective on all 7 Henry classification types</sub></td>
</tr>
</table>

### Qualitative Results

<div align=\"center\">

| Template S<sub>a</sub> | Distorted S<sub>b</sub> | Aligned S<sub>b</sub>' | Deformation Field Î¦ |
|:----------------------:|:-----------------------:|:----------------------:|:-------------------:|
| <img src=\"assets/results/template.png\" width=\"120\"/> | <img src=\"assets/results/distorted.png\" width=\"120\"/> | <img src=\"assets/results/aligned.png\" width=\"120\"/> | <img src=\"assets/results/deformation.png\" width=\"120\"/> |

<sub>Example alignment on a never-before-seen fingerprint pair. The predicted deformation field successfully rectifies non-linear elastic distortion.</sub>

</div>

<br>

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/anonymous/2d-gumnet.git
cd 2d-gumnet

# Create environment
conda create -n gumnet python=3.9 -y
conda activate gumnet

# Install PyTorch (CUDA 11.8)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Install dependencies
pip install -r requirements.txt
```

### Inference

```python
import torch
from gumnet import GumNet2D, load_image, visualize_alignment

# Load pretrained model
model = GumNet2D.from_pretrained(\"gumnet-base\")
model.eval()

# Load fingerprint pair
template = load_image(\"examples/template.png\")      # Reference fingerprint
impression = load_image(\"examples/impression.png\")  # Distorted impression

# Predict alignment
with torch.no_grad():
    aligned, deformation_field = model(template, impression)

# Visualize results
visualize_alignment(template, impression, aligned, deformation_field, 
                    save_path=\"output/alignment_result.png\")
```

### Training

```bash
# Train on synthetic dataset
python train.py \
    --config configs/synthetic.yaml \
    --data_path data/anguli \
    --output_dir checkpoints/

# Resume training
python train.py \
    --config configs/synthetic.yaml \
    --resume checkpoints/latest.pth
```

### Evaluation

```bash
# Evaluate alignment quality
python evaluate.py \
    --checkpoint checkpoints/best_model.pth \
    --data_path data/test \
    --output_dir results/
```

<br>

---

## ğŸ“ Repository Structure

```
2d-gumnet/
â”œâ”€â”€ configs/                  # Training configurations
â”‚   â”œâ”€â”€ synthetic.yaml
â”‚   â””â”€â”€ custom.yaml
â”œâ”€â”€ gumnet/                   # Core library
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py              # GumNet2D architecture
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py
â”‚   â”‚   â”œâ”€â”€ siamese_matching.py
â”‚   â”‚   â”œâ”€â”€ spatial_transformer.py
â”‚   â”‚   â””â”€â”€ dct_pooling.py    # DCT spectral pooling
â”‚   â”œâ”€â”€ losses.py             # Loss functions
â”‚   â””â”€â”€ utils.py              # Utilities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation script
â”‚   â””â”€â”€ visualize.py          # Visualization tools
â”œâ”€â”€ data/                     # Dataset directory
â”œâ”€â”€ checkpoints/              # Model weights
â”œâ”€â”€ assets/                   # README assets
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

<br>

---

## ğŸ“¦ Dataset

We utilize synthetic fingerprints generated with the **Anguli** fingerprint generator:

| Attribute | Details |
|:----------|:--------|
| **Total Images** | 73,500 |
| **SNR Levels** | 5 (varying noise conditions) |
| **Classification Types** | 7 (Henry Classification) |
| **Types** | Arch, Tented Arch, Right Loop, Left Loop, Whorl, Twin Loop, Accidental |

<br>

---

## ğŸ“… Release Timeline

| Asset | Status | Expected |
|:------|:------:|:---------|
| ğŸ“„ Paper | âœ… Submitted | CVPR 2026 |
| ğŸ’» Training Code | â³ Pending | Upon Acceptance |
| ğŸ’» Evaluation Code | â³ Pending | Upon Acceptance |
| ğŸ”§ Pre-trained Weights | â³ Pending | Camera-Ready |
| ğŸ“Š Synthetic Dataset | â³ Pending | Upon Acceptance |
| ğŸ® Demo | â³ Pending | Camera-Ready |

<br>

---

## ğŸ“š Citation

If you find this work useful in your research, please consider citing:

```bibtex
@inproceedings{gumnet2d2026,
    title     = {2D Gum-Net: Unsupervised Dense Deformation Grid Prediction 
                 for Elastic Fingerprint Alignment},
    author    = {Anonymous},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision 
                 and Pattern Recognition (CVPR)},
    year      = {2026}
}
```

<br>

---

## Acknowledgments

This work builds upon the foundational Gum-Net architecture developed for cryo-electron tomography subtomogram alignment. We thank the original authors for their pioneering contributions to unsupervised structural alignment.

<br>

---

## License

This project is released under the [MIT License](LICENSE).

<br>

---

<div align=\"center\">

**Anonymous CVPR 2026 Submission**

<br>

<sub>If you have any questions, please open an issue or contact us after the review process.</sub>

<br>

â­ **Star this repository if you find it helpful!** â­

</div>
"
## Important Links:

- [ICML New In ML Affinity Event](https://newinml.github.io/NewInML2025ICML/)
- [Gum-Net Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Gum-Net_Unsupervised_Geometric_Matching_for_Fast_and_Accurate_3D_Subtomogram_CVPR_2020_paper.pdf)
- [Gum-Net Supplemental](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Zeng_Gum-Net_Unsupervised_Geometric_CVPR_2020_supplemental.pdf#page=3.00)
- [Google Drive](https://drive.google.com/drive/folders/1iDYChJ-Ee0wIsDdZsaEcUtGYgJdXz5y2)
- [Current Gold Standard Fingerprint Matching Methods](https://docs.google.com/spreadsheets/d/1cSLaRhZ0j-iSGLZHZiXBqevQrt3hOLhBJd39syqC32s/edit?usp=sharing)
- [Dates and Deadlines](https://icml.cc/Conferences/2026/Dates) 
- [A Curated List of Fingerprint Datasets.](https://github.com/robertvazan/fingerprint-datasets)
